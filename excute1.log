nohup: 忽略输入
/home/zhang_t/workspace/southner/model/ResAttentionNet.py:250: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  fall_res[:,:,[6,7]] = self.softmax(fall_res[:,:,[6,7]])
/home/zhang_t/workspace/southner/utils/utils.py:102: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/IndexingUtils.h:27.)
  noo_pred_c = noo_pred[noo_pred_mask]
/home/zhang_t/workspace/southner/utils/utils.py:103: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/IndexingUtils.h:27.)
  noo_target_c = noo_target[noo_pred_mask]
/home/zhang_t/.conda/envs/mmwave/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
/home/zhang_t/workspace/southner/utils/utils.py:132: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/IndexingUtils.h:27.)
  box_pred_response = box_pred[coo_response_mask].view(-1,3)
/home/zhang_t/workspace/southner/utils/utils.py:133: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/IndexingUtils.h:27.)
  box_target_response = box_target[coo_response_mask].view(-1,3)
/home/zhang_t/workspace/southner/utils/utils.py:139: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/IndexingUtils.h:27.)
  box_pred_not_response = box_pred[coo_not_response_mask].view(-1,3)
/home/zhang_t/workspace/southner/utils/utils.py:140: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/IndexingUtils.h:27.)
  box_target_not_response = box_target[coo_not_response_mask].view(-1,3)
/home/zhang_t/.conda/envs/mmwave/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/IndexingUtils.h:27.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
epoch:000 || batch 000 with loss:3539.217041 
epoch:000 || batch 010 with loss:3299.462891 
epoch:000 || batch 020 with loss:3339.770508 
epoch:000 || batch 030 with loss:2921.110352 
epoch:000 || batch 040 with loss:2851.008545 
epoch:000 || batch 050 with loss:2841.434570 
epoch:000 || batch 060 with loss:2601.005859 
epoch:000 || batch 070 with loss:2534.194092 
epoch:000 || batch 080 with loss:2561.783936 
epoch:000 || batch 090 with loss:2380.188721 
epoch:001 || batch 000 with loss:2548.796387 
epoch:001 || batch 010 with loss:2121.356201 
epoch:001 || batch 020 with loss:2362.551514 
epoch:001 || batch 030 with loss:2302.916748 
epoch:001 || batch 040 with loss:2051.812500 
epoch:001 || batch 050 with loss:1848.984985 
epoch:001 || batch 060 with loss:2083.406738 
epoch:001 || batch 070 with loss:1775.021240 
epoch:001 || batch 080 with loss:1657.442261 
epoch:001 || batch 090 with loss:1614.937256 
epoch:002 || batch 000 with loss:1852.380859 
epoch:002 || batch 010 with loss:1569.380005 
epoch:002 || batch 020 with loss:1496.918945 
epoch:002 || batch 030 with loss:1662.177368 
epoch:002 || batch 040 with loss:1479.722900 
epoch:002 || batch 050 with loss:1496.174805 
epoch:002 || batch 060 with loss:1314.140381 
epoch:002 || batch 070 with loss:1190.621216 
epoch:002 || batch 080 with loss:1166.235474 
epoch:002 || batch 090 with loss:1206.143433 
epoch:003 || batch 000 with loss:1257.403320 
epoch:003 || batch 010 with loss:1279.293945 
epoch:003 || batch 020 with loss:1035.910278 
epoch:003 || batch 030 with loss:1123.530884 
epoch:003 || batch 040 with loss:924.161377 
epoch:003 || batch 050 with loss:982.747498 
epoch:003 || batch 060 with loss:933.043274 
epoch:003 || batch 070 with loss:910.282776 
epoch:003 || batch 080 with loss:855.859497 
epoch:003 || batch 090 with loss:751.441101 
epoch:004 || batch 000 with loss:1003.640381 
epoch:004 || batch 010 with loss:873.847351 
epoch:004 || batch 020 with loss:692.352600 
epoch:004 || batch 030 with loss:686.787842 
epoch:004 || batch 040 with loss:759.660156 
epoch:004 || batch 050 with loss:701.645447 
epoch:004 || batch 060 with loss:621.748657 
epoch:004 || batch 070 with loss:661.203003 
epoch:004 || batch 080 with loss:505.396149 
epoch:004 || batch 090 with loss:505.458313 
epoch:005 || batch 000 with loss:554.348450 
epoch:005 || batch 010 with loss:533.139282 
epoch:005 || batch 020 with loss:602.823425 
epoch:005 || batch 030 with loss:556.450989 
epoch:005 || batch 040 with loss:497.369720 
epoch:005 || batch 050 with loss:451.894104 
epoch:005 || batch 060 with loss:490.811920 
epoch:005 || batch 070 with loss:512.063232 
epoch:005 || batch 080 with loss:387.228180 
epoch:005 || batch 090 with loss:446.739380 
epoch:006 || batch 000 with loss:453.998993 
epoch:006 || batch 010 with loss:323.100220 
epoch:006 || batch 020 with loss:352.937927 
epoch:006 || batch 030 with loss:392.951965 
epoch:006 || batch 040 with loss:457.659302 
epoch:006 || batch 050 with loss:402.298950 
epoch:006 || batch 060 with loss:357.099243 
epoch:006 || batch 070 with loss:300.964203 
epoch:006 || batch 080 with loss:380.324860 
epoch:006 || batch 090 with loss:352.079712 
epoch:007 || batch 000 with loss:374.014404 
epoch:007 || batch 010 with loss:391.025513 
epoch:007 || batch 020 with loss:357.562378 
epoch:007 || batch 030 with loss:356.407196 
epoch:007 || batch 040 with loss:314.110443 
epoch:007 || batch 050 with loss:282.860168 
epoch:007 || batch 060 with loss:310.591827 
epoch:007 || batch 070 with loss:297.465424 
epoch:007 || batch 080 with loss:265.720123 
epoch:007 || batch 090 with loss:191.808990 
epoch:008 || batch 000 with loss:225.021255 
epoch:008 || batch 010 with loss:285.419128 
epoch:008 || batch 020 with loss:296.631989 
epoch:008 || batch 030 with loss:306.463623 
epoch:008 || batch 040 with loss:276.801788 
epoch:008 || batch 050 with loss:261.798615 
epoch:008 || batch 060 with loss:241.078583 
epoch:008 || batch 070 with loss:248.907104 
epoch:008 || batch 080 with loss:160.111084 
epoch:008 || batch 090 with loss:235.562408 
epoch:009 || batch 000 with loss:206.333466 
epoch:009 || batch 010 with loss:209.370178 
epoch:009 || batch 020 with loss:259.404907 
epoch:009 || batch 030 with loss:214.377594 
epoch:009 || batch 040 with loss:219.978088 
epoch:009 || batch 050 with loss:173.273331 
epoch:009 || batch 060 with loss:214.897034 
epoch:009 || batch 070 with loss:208.215759 
epoch:009 || batch 080 with loss:165.044189 
epoch:009 || batch 090 with loss:220.383804 
epoch:010 || batch 000 with loss:171.807922 
epoch:010 || batch 010 with loss:149.601227 
epoch:010 || batch 020 with loss:183.186783 
epoch:010 || batch 030 with loss:221.186630 
epoch:010 || batch 040 with loss:249.896545 
epoch:010 || batch 050 with loss:154.144608 
epoch:010 || batch 060 with loss:246.872925 
epoch:010 || batch 070 with loss:207.568115 
epoch:010 || batch 080 with loss:200.432251 
epoch:010 || batch 090 with loss:173.548447 
epoch:011 || batch 000 with loss:211.024002 
epoch:011 || batch 010 with loss:201.685547 
epoch:011 || batch 020 with loss:209.562286 
epoch:011 || batch 030 with loss:159.035767 
epoch:011 || batch 040 with loss:176.029617 
epoch:011 || batch 050 with loss:207.733337 
epoch:011 || batch 060 with loss:186.847931 
epoch:011 || batch 070 with loss:218.856781 
epoch:011 || batch 080 with loss:179.545990 
epoch:011 || batch 090 with loss:183.731094 
epoch:012 || batch 000 with loss:197.911438 
epoch:012 || batch 010 with loss:179.350067 
epoch:012 || batch 020 with loss:196.474060 
epoch:012 || batch 030 with loss:171.704926 
epoch:012 || batch 040 with loss:210.047714 
epoch:012 || batch 050 with loss:166.051819 
epoch:012 || batch 060 with loss:186.441818 
epoch:012 || batch 070 with loss:183.290710 
epoch:012 || batch 080 with loss:191.467133 
epoch:012 || batch 090 with loss:154.301575 
epoch:013 || batch 000 with loss:143.420868 
epoch:013 || batch 010 with loss:137.221344 
epoch:013 || batch 020 with loss:190.979691 
epoch:013 || batch 030 with loss:166.471786 
epoch:013 || batch 040 with loss:154.862152 
epoch:013 || batch 050 with loss:184.889832 
epoch:013 || batch 060 with loss:188.885834 
epoch:013 || batch 070 with loss:153.026306 
epoch:013 || batch 080 with loss:185.062759 
epoch:013 || batch 090 with loss:146.230530 
epoch:014 || batch 000 with loss:149.059662 
epoch:014 || batch 010 with loss:141.055466 
epoch:014 || batch 020 with loss:154.778580 
epoch:014 || batch 030 with loss:134.214081 
epoch:014 || batch 040 with loss:163.044281 
epoch:014 || batch 050 with loss:169.361801 
epoch:014 || batch 060 with loss:154.353790 
epoch:014 || batch 070 with loss:124.201111 
epoch:014 || batch 080 with loss:127.145233 
epoch:014 || batch 090 with loss:139.511108 
epoch:015 || batch 000 with loss:158.685822 
epoch:015 || batch 010 with loss:151.130386 
epoch:015 || batch 020 with loss:167.889557 
epoch:015 || batch 030 with loss:173.245834 
epoch:015 || batch 040 with loss:147.643127 
epoch:015 || batch 050 with loss:180.037277 
epoch:015 || batch 060 with loss:182.491638 
epoch:015 || batch 070 with loss:153.996368 
epoch:015 || batch 080 with loss:131.030090 
epoch:015 || batch 090 with loss:152.080414 
epoch:016 || batch 000 with loss:130.468704 
epoch:016 || batch 010 with loss:143.382156 
epoch:016 || batch 020 with loss:142.071289 
epoch:016 || batch 030 with loss:116.865387 
epoch:016 || batch 040 with loss:107.550499 
epoch:016 || batch 050 with loss:152.529434 
epoch:016 || batch 060 with loss:166.937073 
epoch:016 || batch 070 with loss:192.775696 
epoch:016 || batch 080 with loss:169.422440 
epoch:016 || batch 090 with loss:134.170593 
epoch:017 || batch 000 with loss:123.648636 
epoch:017 || batch 010 with loss:159.536911 
epoch:017 || batch 020 with loss:136.579102 
epoch:017 || batch 030 with loss:138.092926 
epoch:017 || batch 040 with loss:122.698318 
epoch:017 || batch 050 with loss:117.944565 
epoch:017 || batch 060 with loss:121.767113 
epoch:017 || batch 070 with loss:121.170380 
epoch:017 || batch 080 with loss:127.166328 
epoch:017 || batch 090 with loss:135.958923 
epoch:018 || batch 000 with loss:109.928940 
epoch:018 || batch 010 with loss:139.900299 
epoch:018 || batch 020 with loss:124.770554 
epoch:018 || batch 030 with loss:148.609528 
epoch:018 || batch 040 with loss:124.971367 
epoch:018 || batch 050 with loss:147.530197 
epoch:018 || batch 060 with loss:128.333527 
epoch:018 || batch 070 with loss:146.324829 
epoch:018 || batch 080 with loss:114.503510 
epoch:018 || batch 090 with loss:124.706848 
epoch:019 || batch 000 with loss:137.375549 
epoch:019 || batch 010 with loss:166.279083 
epoch:019 || batch 020 with loss:157.881577 
epoch:019 || batch 030 with loss:125.388954 
epoch:019 || batch 040 with loss:130.023071 
epoch:019 || batch 050 with loss:136.730621 
epoch:019 || batch 060 with loss:121.248474 
epoch:019 || batch 070 with loss:145.108429 
epoch:019 || batch 080 with loss:125.824440 
epoch:019 || batch 090 with loss:156.362366 
epoch:020 || batch 000 with loss:135.786423 
epoch:020 || batch 010 with loss:123.445511 
epoch:020 || batch 020 with loss:142.210052 
epoch:020 || batch 030 with loss:124.155853 
epoch:020 || batch 040 with loss:140.715408 
epoch:020 || batch 050 with loss:127.569550 
epoch:020 || batch 060 with loss:126.300880 
epoch:020 || batch 070 with loss:138.229477 
epoch:020 || batch 080 with loss:152.324554 
epoch:020 || batch 090 with loss:125.249725 
epoch:021 || batch 000 with loss:135.045059 
epoch:021 || batch 010 with loss:134.703049 
epoch:021 || batch 020 with loss:139.547165 
epoch:021 || batch 030 with loss:144.630981 
epoch:021 || batch 040 with loss:111.484383 
epoch:021 || batch 050 with loss:136.464371 
epoch:021 || batch 060 with loss:114.470497 
epoch:021 || batch 070 with loss:105.934967 
epoch:021 || batch 080 with loss:104.472427 
epoch:021 || batch 090 with loss:147.505920 
epoch:022 || batch 000 with loss:111.369080 
epoch:022 || batch 010 with loss:131.143250 
epoch:022 || batch 020 with loss:141.525909 
epoch:022 || batch 030 with loss:115.983376 
epoch:022 || batch 040 with loss:107.038376 
epoch:022 || batch 050 with loss:116.250809 
epoch:022 || batch 060 with loss:152.253479 
epoch:022 || batch 070 with loss:118.512177 
epoch:022 || batch 080 with loss:106.908493 
epoch:022 || batch 090 with loss:124.104744 
epoch:023 || batch 000 with loss:108.231796 
epoch:023 || batch 010 with loss:124.648117 
epoch:023 || batch 020 with loss:117.066498 
epoch:023 || batch 030 with loss:110.994812 
epoch:023 || batch 040 with loss:127.900284 
epoch:023 || batch 050 with loss:107.342285 
epoch:023 || batch 060 with loss:113.071228 
epoch:023 || batch 070 with loss:124.785965 
epoch:023 || batch 080 with loss:122.027435 
epoch:023 || batch 090 with loss:124.902023 
epoch:024 || batch 000 with loss:118.699875 
epoch:024 || batch 010 with loss:138.827103 
epoch:024 || batch 020 with loss:139.430481 
epoch:024 || batch 030 with loss:113.175278 
epoch:024 || batch 040 with loss:121.755608 
epoch:024 || batch 050 with loss:114.159973 
epoch:024 || batch 060 with loss:109.084290 
epoch:024 || batch 070 with loss:116.507957 
epoch:024 || batch 080 with loss:105.782990 
epoch:024 || batch 090 with loss:119.808342 
epoch:025 || batch 000 with loss:152.179001 
epoch:025 || batch 010 with loss:115.858025 
epoch:025 || batch 020 with loss:114.378387 
epoch:025 || batch 030 with loss:113.590317 
epoch:025 || batch 040 with loss:131.966553 
epoch:025 || batch 050 with loss:114.416885 
epoch:025 || batch 060 with loss:114.224525 
epoch:025 || batch 070 with loss:107.519913 
epoch:025 || batch 080 with loss:112.603561 
epoch:025 || batch 090 with loss:128.856873 
epoch:026 || batch 000 with loss:105.790970 
epoch:026 || batch 010 with loss:108.670891 
epoch:026 || batch 020 with loss:103.339935 
epoch:026 || batch 030 with loss:112.653900 
epoch:026 || batch 040 with loss:119.684120 
epoch:026 || batch 050 with loss:116.034187 
epoch:026 || batch 060 with loss:106.005432 
epoch:026 || batch 070 with loss:120.163284 
epoch:026 || batch 080 with loss:113.651360 
epoch:026 || batch 090 with loss:112.755142 
epoch:027 || batch 000 with loss:111.555557 
epoch:027 || batch 010 with loss:119.843597 
epoch:027 || batch 020 with loss:114.658028 
epoch:027 || batch 030 with loss:134.571198 
epoch:027 || batch 040 with loss:112.327888 
epoch:027 || batch 050 with loss:120.571609 
epoch:027 || batch 060 with loss:114.388229 
epoch:027 || batch 070 with loss:124.395325 
epoch:027 || batch 080 with loss:118.829987 
epoch:027 || batch 090 with loss:107.976456 
/home/zhang_t/.conda/envs/mmwave/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: Error detected in MseLossBackward0. Traceback of forward call that caused the error:
  File "/home/zhang_t/workspace/southner/train.py", line 202, in <module>
    main()
  File "/home/zhang_t/workspace/southner/train.py", line 110, in main
    train(e, model, train_data_loader, criterion, optimizer)
  File "/home/zhang_t/workspace/southner/train.py", line 141, in train
    loss = criterion(predict, tag)
  File "/home/zhang_t/.conda/envs/mmwave/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zhang_t/workspace/southner/utils/utils.py", line 145, in forward
    class_loss = F.mse_loss(class_pred,class_target,size_average=False)
  File "/home/zhang_t/.conda/envs/mmwave/lib/python3.9/site-packages/torch/nn/functional.py", line 3292, in mse_loss
    return torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
  File "/home/zhang_t/.conda/envs/mmwave/lib/python3.9/site-packages/torch/fx/traceback.py", line 57, in format_stack
    return traceback.format_stack()
 (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/torch/csrc/autograd/python_anomaly_mode.cpp:114.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
epoch:028 || batch 000 with loss:117.397644 
Traceback (most recent call last):
  File "/home/zhang_t/workspace/southner/train.py", line 202, in <module>
    main()
  File "/home/zhang_t/workspace/southner/train.py", line 110, in main
    train(e, model, train_data_loader, criterion, optimizer)
  File "/home/zhang_t/workspace/southner/train.py", line 144, in train
    loss.backward()
  File "/home/zhang_t/.conda/envs/mmwave/lib/python3.9/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/home/zhang_t/.conda/envs/mmwave/lib/python3.9/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Function 'MseLossBackward0' returned nan values in its 0th output.
