nohup: 忽略输入
/home/zhang_t/workspace/southner/model/ResAttentionNet.py:252: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  fall_res[:,:,[6,7]] = self.softmax(fall_res[:,:,[6,7]])
/home/zhang_t/workspace/southner/utils/utils.py:103: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/IndexingUtils.h:27.)
  noo_pred_c = noo_pred[noo_pred_mask]
/home/zhang_t/workspace/southner/utils/utils.py:104: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/IndexingUtils.h:27.)
  noo_target_c = noo_target[noo_pred_mask]
/home/zhang_t/.conda/envs/mmwave/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
/home/zhang_t/workspace/southner/utils/utils.py:133: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/IndexingUtils.h:27.)
  box_pred_response = box_pred[coo_response_mask].view(-1,3)
/home/zhang_t/workspace/southner/utils/utils.py:134: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/IndexingUtils.h:27.)
  box_target_response = box_target[coo_response_mask].view(-1,3)
/home/zhang_t/workspace/southner/utils/utils.py:140: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/IndexingUtils.h:27.)
  box_pred_not_response = box_pred[coo_not_response_mask].view(-1,3)
/home/zhang_t/workspace/southner/utils/utils.py:141: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/IndexingUtils.h:27.)
  box_target_not_response = box_target[coo_not_response_mask].view(-1,3)
/home/zhang_t/.conda/envs/mmwave/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/IndexingUtils.h:27.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/zhang_t/workspace/southner/show_res.py:22: RuntimeWarning: divide by zero encountered in log
  return (np.log(data))/(max_val+1)*255
epoch:000 || batch 000 with loss:838.632385 
epoch:000 || batch 010 with loss:732.985657 
epoch:000 || batch 020 with loss:484.318359 
epoch:000 || batch 030 with loss:598.335266 
epoch:000 || batch 040 with loss:520.456543 
epoch:000 || batch 050 with loss:592.851624 
epoch:000 || batch 060 with loss:422.693420 
epoch:000 || batch 070 with loss:455.512238 
epoch:000 || batch 080 with loss:352.047089 
epoch:000 || batch 090 with loss:432.903992 
epoch:000 || batch 100 with loss:293.727478 
epoch:000 || batch 110 with loss:557.123840 
epoch:000 || batch 120 with loss:330.029846 
epoch:000 || batch 130 with loss:311.064819 
epoch:000 || batch 140 with loss:381.503357 
epoch:000 || batch 150 with loss:371.001678 
epoch:000 || batch 160 with loss:364.106750 
epoch:000 || batch 170 with loss:440.595398 
epoch:000 || batch 180 with loss:293.069061 
epoch:000 || batch 190 with loss:302.152405 
epoch:000 || batch 200 with loss:362.505219 
epoch:000 || batch 210 with loss:327.455566 
epoch:000 || batch 220 with loss:366.810120 
epoch:000 || batch 230 with loss:320.759430 
epoch:000 || batch 240 with loss:321.904388 
epoch:000 || batch 250 with loss:417.412384 
epoch:000 || batch 260 with loss:267.599030 
epoch:000 || batch 270 with loss:267.012665 
epoch:000 || batch 280 with loss:346.630981 
epoch:000 || batch 290 with loss:265.427002 
epoch:000 || batch 300 with loss:384.243835 
epoch:000 || batch 310 with loss:288.224396 
epoch:000 || batch 320 with loss:293.660553 
epoch:000 || batch 330 with loss:378.311890 
epoch:000 || batch 340 with loss:390.331390 
epoch:000 || batch 350 with loss:346.640930 
epoch:000 || batch 360 with loss:327.232422 
epoch:000 || batch 370 with loss:351.176056 
epoch:000 || batch 380 with loss:318.118286 
epoch:001 || batch 000 with loss:425.637054 
epoch:001 || batch 010 with loss:314.557373 
epoch:001 || batch 020 with loss:325.866119 
epoch:001 || batch 030 with loss:256.103943 
epoch:001 || batch 040 with loss:314.329712 
epoch:001 || batch 050 with loss:233.754044 
epoch:001 || batch 060 with loss:205.873322 
epoch:001 || batch 070 with loss:323.607880 
epoch:001 || batch 080 with loss:358.797852 
epoch:001 || batch 090 with loss:299.341827 
epoch:001 || batch 100 with loss:339.450043 
epoch:001 || batch 110 with loss:273.974182 
epoch:001 || batch 120 with loss:315.994690 
epoch:001 || batch 130 with loss:338.370514 
epoch:001 || batch 140 with loss:249.990479 
epoch:001 || batch 150 with loss:247.531219 
epoch:001 || batch 160 with loss:297.041595 
epoch:001 || batch 170 with loss:345.889587 
epoch:001 || batch 180 with loss:299.604553 
epoch:001 || batch 190 with loss:307.385803 
epoch:001 || batch 200 with loss:346.698334 
epoch:001 || batch 210 with loss:439.152466 
epoch:001 || batch 220 with loss:348.710022 
epoch:001 || batch 230 with loss:248.392654 
epoch:001 || batch 240 with loss:321.939728 
epoch:001 || batch 250 with loss:254.621460 
epoch:001 || batch 260 with loss:237.554688 
epoch:001 || batch 270 with loss:318.586823 
epoch:001 || batch 280 with loss:285.127014 
epoch:001 || batch 290 with loss:328.442505 
epoch:001 || batch 300 with loss:255.878235 
epoch:001 || batch 310 with loss:305.816284 
epoch:001 || batch 320 with loss:282.172699 
epoch:001 || batch 330 with loss:359.592651 
epoch:001 || batch 340 with loss:298.916748 
epoch:001 || batch 350 with loss:333.578033 
epoch:001 || batch 360 with loss:321.728546 
epoch:001 || batch 370 with loss:363.590393 
epoch:001 || batch 380 with loss:266.122253 
epoch:002 || batch 000 with loss:247.036270 
epoch:002 || batch 010 with loss:322.770996 
epoch:002 || batch 020 with loss:337.254669 
epoch:002 || batch 030 with loss:282.659119 
epoch:002 || batch 040 with loss:309.997070 
epoch:002 || batch 050 with loss:291.922943 
epoch:002 || batch 060 with loss:309.101288 
epoch:002 || batch 070 with loss:411.086761 
epoch:002 || batch 080 with loss:362.851227 
epoch:002 || batch 090 with loss:201.334213 
epoch:002 || batch 100 with loss:361.369507 
epoch:002 || batch 110 with loss:272.344269 
epoch:002 || batch 120 with loss:330.723694 
epoch:002 || batch 130 with loss:261.495331 
epoch:002 || batch 140 with loss:437.459045 
epoch:002 || batch 150 with loss:237.363113 
epoch:002 || batch 160 with loss:269.479858 
epoch:002 || batch 170 with loss:313.556854 
epoch:002 || batch 180 with loss:255.443054 
epoch:002 || batch 190 with loss:282.304108 
epoch:002 || batch 200 with loss:343.640289 
epoch:002 || batch 210 with loss:313.748383 
epoch:002 || batch 220 with loss:523.834473 
epoch:002 || batch 230 with loss:259.646210 
epoch:002 || batch 240 with loss:299.803040 
epoch:002 || batch 250 with loss:331.928497 
epoch:002 || batch 260 with loss:280.770660 
epoch:002 || batch 270 with loss:358.788147 
epoch:002 || batch 280 with loss:386.884613 
epoch:002 || batch 290 with loss:231.899811 
epoch:002 || batch 300 with loss:215.294724 
epoch:002 || batch 310 with loss:253.745758 
epoch:002 || batch 320 with loss:267.556885 
epoch:002 || batch 330 with loss:317.911682 
epoch:002 || batch 340 with loss:324.283478 
epoch:002 || batch 350 with loss:249.551147 
epoch:002 || batch 360 with loss:376.707336 
epoch:002 || batch 370 with loss:288.794617 
epoch:002 || batch 380 with loss:373.281311 
epoch:003 || batch 000 with loss:243.327713 
epoch:003 || batch 010 with loss:232.922394 
epoch:003 || batch 020 with loss:214.731781 
epoch:003 || batch 030 with loss:251.055481 
epoch:003 || batch 040 with loss:262.116455 
epoch:003 || batch 050 with loss:397.278809 
epoch:003 || batch 060 with loss:224.735504 
epoch:003 || batch 070 with loss:282.919006 
epoch:003 || batch 080 with loss:388.319611 
epoch:003 || batch 090 with loss:310.775543 
epoch:003 || batch 100 with loss:280.648682 
epoch:003 || batch 110 with loss:246.656067 
epoch:003 || batch 120 with loss:244.146515 
epoch:003 || batch 130 with loss:384.189514 
epoch:003 || batch 140 with loss:277.463409 
epoch:003 || batch 150 with loss:319.183502 
epoch:003 || batch 160 with loss:268.465485 
epoch:003 || batch 170 with loss:279.280670 
epoch:003 || batch 180 with loss:349.128296 
epoch:003 || batch 190 with loss:310.432800 
epoch:003 || batch 200 with loss:356.492157 
epoch:003 || batch 210 with loss:244.576141 
epoch:003 || batch 220 with loss:268.351562 
epoch:003 || batch 230 with loss:212.610703 
epoch:003 || batch 240 with loss:218.106293 
epoch:003 || batch 250 with loss:192.866302 
epoch:003 || batch 260 with loss:381.771667 
epoch:003 || batch 270 with loss:298.141571 
epoch:003 || batch 280 with loss:290.671967 
epoch:003 || batch 290 with loss:348.288086 
epoch:003 || batch 300 with loss:220.198502 
epoch:003 || batch 310 with loss:280.957031 
epoch:003 || batch 320 with loss:197.314621 
epoch:003 || batch 330 with loss:294.404419 
epoch:003 || batch 340 with loss:245.504578 
epoch:003 || batch 350 with loss:207.757492 
epoch:003 || batch 360 with loss:269.427917 
epoch:003 || batch 370 with loss:359.327545 
epoch:003 || batch 380 with loss:307.758301 
epoch:004 || batch 000 with loss:353.128326 
epoch:004 || batch 010 with loss:292.851532 
epoch:004 || batch 020 with loss:251.319244 
epoch:004 || batch 030 with loss:219.386246 
epoch:004 || batch 040 with loss:262.458740 
epoch:004 || batch 050 with loss:297.769928 
epoch:004 || batch 060 with loss:235.004166 
epoch:004 || batch 070 with loss:459.235748 
epoch:004 || batch 080 with loss:218.869812 
epoch:004 || batch 090 with loss:266.128845 
epoch:004 || batch 100 with loss:316.547150 
epoch:004 || batch 110 with loss:319.425934 
epoch:004 || batch 120 with loss:304.249969 
epoch:004 || batch 130 with loss:292.267242 
epoch:004 || batch 140 with loss:213.872116 
epoch:004 || batch 150 with loss:278.835693 
epoch:004 || batch 160 with loss:348.574951 
epoch:004 || batch 170 with loss:262.720062 
epoch:004 || batch 180 with loss:294.594116 
epoch:004 || batch 190 with loss:293.787567 
epoch:004 || batch 200 with loss:337.153687 
epoch:004 || batch 210 with loss:249.585571 
epoch:004 || batch 220 with loss:303.571930 
epoch:004 || batch 230 with loss:277.631531 
epoch:004 || batch 240 with loss:309.339325 
epoch:004 || batch 250 with loss:257.932709 
epoch:004 || batch 260 with loss:301.114716 
epoch:004 || batch 270 with loss:424.741425 
epoch:004 || batch 280 with loss:272.976990 
epoch:004 || batch 290 with loss:318.838196 
epoch:004 || batch 300 with loss:215.536331 
epoch:004 || batch 310 with loss:288.310547 
epoch:004 || batch 320 with loss:312.235168 
epoch:004 || batch 330 with loss:366.829498 
epoch:004 || batch 340 with loss:285.826721 
epoch:004 || batch 350 with loss:235.541428 
epoch:004 || batch 360 with loss:207.400299 
epoch:004 || batch 370 with loss:320.869873 
epoch:004 || batch 380 with loss:232.276367 
epoch:005 || batch 000 with loss:335.653595 
epoch:005 || batch 010 with loss:273.020447 
epoch:005 || batch 020 with loss:256.809418 
epoch:005 || batch 030 with loss:356.835693 
epoch:005 || batch 040 with loss:264.116760 
epoch:005 || batch 050 with loss:267.471924 
epoch:005 || batch 060 with loss:265.738770 
epoch:005 || batch 070 with loss:456.174774 
epoch:005 || batch 080 with loss:283.828766 
epoch:005 || batch 090 with loss:275.540863 
epoch:005 || batch 100 with loss:307.397766 
epoch:005 || batch 110 with loss:252.654068 
epoch:005 || batch 120 with loss:316.136902 
epoch:005 || batch 130 with loss:298.110931 
epoch:005 || batch 140 with loss:215.259918 
epoch:005 || batch 150 with loss:428.504883 
epoch:005 || batch 160 with loss:327.479218 
epoch:005 || batch 170 with loss:383.622406 
epoch:005 || batch 180 with loss:277.644104 
epoch:005 || batch 190 with loss:259.305664 
epoch:005 || batch 200 with loss:276.949005 
epoch:005 || batch 210 with loss:283.666351 
epoch:005 || batch 220 with loss:183.985870 
epoch:005 || batch 230 with loss:200.527939 
epoch:005 || batch 240 with loss:333.283295 
epoch:005 || batch 250 with loss:346.766907 
epoch:005 || batch 260 with loss:250.121704 
epoch:005 || batch 270 with loss:230.834457 
epoch:005 || batch 280 with loss:202.269333 
epoch:005 || batch 290 with loss:284.567383 
epoch:005 || batch 300 with loss:298.574738 
epoch:005 || batch 310 with loss:265.375671 
epoch:005 || batch 320 with loss:227.031952 
epoch:005 || batch 330 with loss:367.967834 
epoch:005 || batch 340 with loss:245.307739 
epoch:005 || batch 350 with loss:333.075928 
epoch:005 || batch 360 with loss:330.276123 
epoch:005 || batch 370 with loss:304.951538 
epoch:005 || batch 380 with loss:300.101654 
epoch:006 || batch 000 with loss:252.610855 
epoch:006 || batch 010 with loss:290.746155 
epoch:006 || batch 020 with loss:386.476379 
epoch:006 || batch 030 with loss:235.075089 
epoch:006 || batch 040 with loss:260.819824 
epoch:006 || batch 050 with loss:347.722595 
epoch:006 || batch 060 with loss:406.538086 
epoch:006 || batch 070 with loss:369.969055 
epoch:006 || batch 080 with loss:343.999756 
epoch:006 || batch 090 with loss:335.607361 
epoch:006 || batch 100 with loss:241.720169 
epoch:006 || batch 110 with loss:376.958954 
epoch:006 || batch 120 with loss:380.256622 
epoch:006 || batch 130 with loss:332.760742 
epoch:006 || batch 140 with loss:221.324997 
epoch:006 || batch 150 with loss:304.745789 
epoch:006 || batch 160 with loss:320.467987 
epoch:006 || batch 170 with loss:358.809875 
epoch:006 || batch 180 with loss:292.460297 
epoch:006 || batch 190 with loss:284.891815 
epoch:006 || batch 200 with loss:276.907013 
epoch:006 || batch 210 with loss:261.461426 
epoch:006 || batch 220 with loss:312.670654 
epoch:006 || batch 230 with loss:233.130402 
epoch:006 || batch 240 with loss:290.673584 
epoch:006 || batch 250 with loss:280.762543 
epoch:006 || batch 260 with loss:369.643097 
epoch:006 || batch 270 with loss:210.086273 
epoch:006 || batch 280 with loss:206.275772 
epoch:006 || batch 290 with loss:211.068985 
epoch:006 || batch 300 with loss:217.837204 
epoch:006 || batch 310 with loss:271.846558 
epoch:006 || batch 320 with loss:320.749542 
epoch:006 || batch 330 with loss:276.026611 
epoch:006 || batch 340 with loss:294.802429 
epoch:006 || batch 350 with loss:234.395584 
epoch:006 || batch 360 with loss:275.617279 
epoch:006 || batch 370 with loss:292.253174 
epoch:006 || batch 380 with loss:265.699371 
epoch:007 || batch 000 with loss:258.439209 
epoch:007 || batch 010 with loss:310.296478 
epoch:007 || batch 020 with loss:258.337036 
epoch:007 || batch 030 with loss:286.673035 
epoch:007 || batch 040 with loss:350.503876 
epoch:007 || batch 050 with loss:247.128052 
epoch:007 || batch 060 with loss:267.128326 
epoch:007 || batch 070 with loss:188.959488 
epoch:007 || batch 080 with loss:272.934601 
epoch:007 || batch 090 with loss:300.808533 
epoch:007 || batch 100 with loss:225.471527 
epoch:007 || batch 110 with loss:235.723389 
epoch:007 || batch 120 with loss:242.721680 
epoch:007 || batch 130 with loss:287.014099 
epoch:007 || batch 140 with loss:280.955231 
epoch:007 || batch 150 with loss:232.510620 
epoch:007 || batch 160 with loss:248.864059 
epoch:007 || batch 170 with loss:280.328857 
epoch:007 || batch 180 with loss:216.780884 
epoch:007 || batch 190 with loss:285.010712 
epoch:007 || batch 200 with loss:295.144318 
epoch:007 || batch 210 with loss:336.867737 
epoch:007 || batch 220 with loss:322.447784 
epoch:007 || batch 230 with loss:366.111328 
epoch:007 || batch 240 with loss:313.804199 
epoch:007 || batch 250 with loss:207.547302 
epoch:007 || batch 260 with loss:308.881073 
epoch:007 || batch 270 with loss:292.226166 
epoch:007 || batch 280 with loss:265.978058 
epoch:007 || batch 290 with loss:238.427551 
epoch:007 || batch 300 with loss:219.255981 
epoch:007 || batch 310 with loss:219.334503 
epoch:007 || batch 320 with loss:318.889435 
epoch:007 || batch 330 with loss:225.052261 
epoch:007 || batch 340 with loss:279.305054 
epoch:007 || batch 350 with loss:235.283279 
epoch:007 || batch 360 with loss:303.371094 
epoch:007 || batch 370 with loss:273.750610 
epoch:007 || batch 380 with loss:297.901855 
epoch:008 || batch 000 with loss:318.638092 
epoch:008 || batch 010 with loss:284.142273 
epoch:008 || batch 020 with loss:290.953308 
epoch:008 || batch 030 with loss:263.581909 
epoch:008 || batch 040 with loss:296.944000 
epoch:008 || batch 050 with loss:271.291840 
epoch:008 || batch 060 with loss:306.197632 
epoch:008 || batch 070 with loss:260.715363 
epoch:008 || batch 080 with loss:305.588654 
epoch:008 || batch 090 with loss:298.666748 
epoch:008 || batch 100 with loss:331.435303 
epoch:008 || batch 110 with loss:338.305481 
epoch:008 || batch 120 with loss:219.918503 
epoch:008 || batch 130 with loss:263.344513 
epoch:008 || batch 140 with loss:196.618286 
epoch:008 || batch 150 with loss:256.173553 
epoch:008 || batch 160 with loss:259.490417 
epoch:008 || batch 170 with loss:280.921967 
epoch:008 || batch 180 with loss:230.826965 
epoch:008 || batch 190 with loss:225.857147 
epoch:008 || batch 200 with loss:223.507065 
epoch:008 || batch 210 with loss:326.572540 
epoch:008 || batch 220 with loss:217.222687 
epoch:008 || batch 230 with loss:225.352432 
epoch:008 || batch 240 with loss:315.990448 
epoch:008 || batch 250 with loss:255.941742 
epoch:008 || batch 260 with loss:268.231659 
epoch:008 || batch 270 with loss:230.032318 
epoch:008 || batch 280 with loss:213.915466 
epoch:008 || batch 290 with loss:256.435944 
epoch:008 || batch 300 with loss:207.648926 
epoch:008 || batch 310 with loss:290.671234 
epoch:008 || batch 320 with loss:273.241211 
epoch:008 || batch 330 with loss:252.750610 
epoch:008 || batch 340 with loss:320.679474 
epoch:008 || batch 350 with loss:253.151825 
epoch:008 || batch 360 with loss:275.122772 
epoch:008 || batch 370 with loss:273.897552 
epoch:008 || batch 380 with loss:248.013794 
epoch:009 || batch 000 with loss:280.280975 
epoch:009 || batch 010 with loss:291.597687 
epoch:009 || batch 020 with loss:217.836502 
epoch:009 || batch 030 with loss:306.968597 
epoch:009 || batch 040 with loss:294.938599 
epoch:009 || batch 050 with loss:328.950317 
epoch:009 || batch 060 with loss:291.580109 
epoch:009 || batch 070 with loss:272.567810 
epoch:009 || batch 080 with loss:308.608765 
epoch:009 || batch 090 with loss:325.803833 
epoch:009 || batch 100 with loss:233.653870 
epoch:009 || batch 110 with loss:242.678604 
epoch:009 || batch 120 with loss:253.034790 
epoch:009 || batch 130 with loss:268.885681 
epoch:009 || batch 140 with loss:257.348114 
epoch:009 || batch 150 with loss:345.966736 
epoch:009 || batch 160 with loss:240.077805 
epoch:009 || batch 170 with loss:368.936432 
epoch:009 || batch 180 with loss:365.584259 
epoch:009 || batch 190 with loss:237.428650 
epoch:009 || batch 200 with loss:310.943054 
epoch:009 || batch 210 with loss:433.072510 
epoch:009 || batch 220 with loss:314.380157 
epoch:009 || batch 230 with loss:252.459900 
epoch:009 || batch 240 with loss:383.331390 
epoch:009 || batch 250 with loss:259.128357 
epoch:009 || batch 260 with loss:319.082306 
epoch:009 || batch 270 with loss:262.518707 
epoch:009 || batch 280 with loss:353.135132 
epoch:009 || batch 290 with loss:282.777374 
epoch:009 || batch 300 with loss:263.402740 
epoch:009 || batch 310 with loss:238.659927 
epoch:009 || batch 320 with loss:274.879089 
epoch:009 || batch 330 with loss:247.977570 
epoch:009 || batch 340 with loss:231.881042 
epoch:009 || batch 350 with loss:329.157898 
epoch:009 || batch 360 with loss:303.198151 
epoch:009 || batch 370 with loss:293.756226 
epoch:009 || batch 380 with loss:373.675568 
epoch:010 || batch 000 with loss:296.829590 
epoch:010 || batch 010 with loss:324.255859 
epoch:010 || batch 020 with loss:279.838013 
epoch:010 || batch 030 with loss:303.091614 
epoch:010 || batch 040 with loss:351.351837 
epoch:010 || batch 050 with loss:311.046814 
epoch:010 || batch 060 with loss:277.535522 
epoch:010 || batch 070 with loss:297.786743 
epoch:010 || batch 080 with loss:316.946381 
epoch:010 || batch 090 with loss:315.618683 
epoch:010 || batch 100 with loss:332.844604 
epoch:010 || batch 110 with loss:332.042725 
epoch:010 || batch 120 with loss:227.821747 
epoch:010 || batch 130 with loss:252.018219 
epoch:010 || batch 140 with loss:229.623169 
epoch:010 || batch 150 with loss:222.910736 
epoch:010 || batch 160 with loss:277.158203 
epoch:010 || batch 170 with loss:285.290710 
epoch:010 || batch 180 with loss:255.384735 
epoch:010 || batch 190 with loss:288.907898 
epoch:010 || batch 200 with loss:308.521179 
epoch:010 || batch 210 with loss:337.984863 
epoch:010 || batch 220 with loss:299.199493 
epoch:010 || batch 230 with loss:222.155045 
epoch:010 || batch 240 with loss:324.193817 
epoch:010 || batch 250 with loss:202.433945 
epoch:010 || batch 260 with loss:211.989212 
epoch:010 || batch 270 with loss:417.491394 
epoch:010 || batch 280 with loss:290.976654 
epoch:010 || batch 290 with loss:254.622726 
epoch:010 || batch 300 with loss:283.982117 
epoch:010 || batch 310 with loss:234.554413 
epoch:010 || batch 320 with loss:360.319550 
epoch:010 || batch 330 with loss:350.784760 
epoch:010 || batch 340 with loss:350.329498 
epoch:010 || batch 350 with loss:312.784485 
epoch:010 || batch 360 with loss:365.051361 
epoch:010 || batch 370 with loss:257.106720 
epoch:010 || batch 380 with loss:416.565735 
epoch:011 || batch 000 with loss:258.777344 
epoch:011 || batch 010 with loss:235.943329 
epoch:011 || batch 020 with loss:359.988708 
epoch:011 || batch 030 with loss:272.476410 
epoch:011 || batch 040 with loss:422.992371 
epoch:011 || batch 050 with loss:310.129822 
epoch:011 || batch 060 with loss:333.286285 
epoch:011 || batch 070 with loss:244.197708 
epoch:011 || batch 080 with loss:203.170303 
epoch:011 || batch 090 with loss:227.540070 
epoch:011 || batch 100 with loss:258.631409 
epoch:011 || batch 110 with loss:304.889404 
epoch:011 || batch 120 with loss:205.916534 
epoch:011 || batch 130 with loss:243.487396 
epoch:011 || batch 140 with loss:279.747192 
epoch:011 || batch 150 with loss:253.928162 
epoch:011 || batch 160 with loss:324.405762 
epoch:011 || batch 170 with loss:289.260437 
epoch:011 || batch 180 with loss:309.263336 
epoch:011 || batch 190 with loss:277.151794 
epoch:011 || batch 200 with loss:306.882477 
epoch:011 || batch 210 with loss:278.647461 
epoch:011 || batch 220 with loss:281.846497 
epoch:011 || batch 230 with loss:226.488312 
epoch:011 || batch 240 with loss:271.617126 
epoch:011 || batch 250 with loss:205.376556 
epoch:011 || batch 260 with loss:317.347198 
epoch:011 || batch 270 with loss:309.680634 
epoch:011 || batch 280 with loss:284.619293 
epoch:011 || batch 290 with loss:247.396957 
epoch:011 || batch 300 with loss:229.674438 
epoch:011 || batch 310 with loss:277.423462 
epoch:011 || batch 320 with loss:257.899231 
epoch:011 || batch 330 with loss:243.655731 
epoch:011 || batch 340 with loss:246.135452 
epoch:011 || batch 350 with loss:273.092621 
epoch:011 || batch 360 with loss:204.677200 
epoch:011 || batch 370 with loss:358.967560 
epoch:011 || batch 380 with loss:289.256348 
/home/zhang_t/.conda/envs/mmwave/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: Error detected in MulBackward0. Traceback of forward call that caused the error:
  File "/home/zhang_t/workspace/southner/train.py", line 174, in <module>
    main()
  File "/home/zhang_t/workspace/southner/train.py", line 81, in main
    train(e, model, train_data_loader, criterion, optimizer)
  File "/home/zhang_t/workspace/southner/train.py", line 111, in train
    loss = criterion(predict, tag)
  File "/home/zhang_t/.conda/envs/mmwave/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zhang_t/workspace/southner/utils/utils.py", line 153, in forward
    total_loss = (self.l_coord*loc_loss + class_loss*contain_loss + not_contain_loss + self.l_noobj*nooobj_loss + self.class_scale*class_loss)+1e-6
  File "/home/zhang_t/.conda/envs/mmwave/lib/python3.9/site-packages/torch/fx/traceback.py", line 57, in format_stack
    return traceback.format_stack()
 (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/torch/csrc/autograd/python_anomaly_mode.cpp:114.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
epoch:012 || batch 000 with loss:280.927826 
epoch:012 || batch 010 with loss:196.286163 
epoch:012 || batch 020 with loss:236.740524 
epoch:012 || batch 030 with loss:229.931442 
epoch:012 || batch 040 with loss:290.660675 
epoch:012 || batch 050 with loss:338.409454 
epoch:012 || batch 060 with loss:351.191711 
epoch:012 || batch 070 with loss:257.088379 
epoch:012 || batch 080 with loss:235.096375 
epoch:012 || batch 090 with loss:241.829132 
epoch:012 || batch 100 with loss:254.919800 
epoch:012 || batch 110 with loss:283.267670 
epoch:012 || batch 120 with loss:193.812683 
epoch:012 || batch 130 with loss:347.669434 
epoch:012 || batch 140 with loss:324.408325 
epoch:012 || batch 150 with loss:294.995728 
epoch:012 || batch 160 with loss:339.725708 
Traceback (most recent call last):
  File "/home/zhang_t/workspace/southner/train.py", line 174, in <module>
    main()
  File "/home/zhang_t/workspace/southner/train.py", line 81, in main
    train(e, model, train_data_loader, criterion, optimizer)
  File "/home/zhang_t/workspace/southner/train.py", line 114, in train
    loss.backward()
  File "/home/zhang_t/.conda/envs/mmwave/lib/python3.9/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/home/zhang_t/.conda/envs/mmwave/lib/python3.9/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Function 'MulBackward0' returned nan values in its 0th output.
