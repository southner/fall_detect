nohup: 忽略输入
/home/zhang_t/workspace/southner/model/ResAttentionNet.py:250: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  fall_res[:,:,[6,7]] = self.softmax(fall_res[:,:,[6,7]])
/home/zhang_t/workspace/southner/utils/utils.py:102: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/IndexingUtils.h:27.)
  noo_pred_c = noo_pred[noo_pred_mask]
/home/zhang_t/workspace/southner/utils/utils.py:103: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/IndexingUtils.h:27.)
  noo_target_c = noo_target[noo_pred_mask]
/home/zhang_t/.conda/envs/mmwave/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
/home/zhang_t/workspace/southner/utils/utils.py:132: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/IndexingUtils.h:27.)
  box_pred_response = box_pred[coo_response_mask].view(-1,3)
/home/zhang_t/workspace/southner/utils/utils.py:133: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/IndexingUtils.h:27.)
  box_target_response = box_target[coo_response_mask].view(-1,3)
/home/zhang_t/workspace/southner/utils/utils.py:139: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/IndexingUtils.h:27.)
  box_pred_not_response = box_pred[coo_not_response_mask].view(-1,3)
/home/zhang_t/workspace/southner/utils/utils.py:140: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/IndexingUtils.h:27.)
  box_target_not_response = box_target[coo_not_response_mask].view(-1,3)
/home/zhang_t/.conda/envs/mmwave/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/IndexingUtils.h:27.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
epoch:000 || batch 000 with loss:3049.324463 
epoch:000 || batch 010 with loss:1932.779053 
epoch:000 || batch 020 with loss:1300.078003 
epoch:000 || batch 030 with loss:856.623169 
epoch:000 || batch 040 with loss:601.882446 
epoch:000 || batch 050 with loss:482.044373 
epoch:000 || batch 060 with loss:341.814514 
epoch:000 || batch 070 with loss:221.113586 
epoch:000 || batch 080 with loss:179.631790 
epoch:000 || batch 090 with loss:157.272034 
epoch:001 || batch 000 with loss:138.378540 
epoch:001 || batch 010 with loss:117.878967 
epoch:001 || batch 020 with loss:131.186569 
epoch:001 || batch 030 with loss:118.825485 
epoch:001 || batch 040 with loss:132.708435 
epoch:001 || batch 050 with loss:115.989487 
epoch:001 || batch 060 with loss:116.545135 
epoch:001 || batch 070 with loss:104.068710 
epoch:001 || batch 080 with loss:107.285797 
epoch:001 || batch 090 with loss:103.337067 
epoch:002 || batch 000 with loss:109.787827 
epoch:002 || batch 010 with loss:103.609077 
epoch:002 || batch 020 with loss:100.196518 
epoch:002 || batch 030 with loss:100.378105 
epoch:002 || batch 040 with loss: 98.263016 
epoch:002 || batch 050 with loss:108.746643 
epoch:002 || batch 060 with loss: 97.186295 
epoch:002 || batch 070 with loss:101.582733 
epoch:002 || batch 080 with loss:100.928085 
epoch:002 || batch 090 with loss: 94.701569 
epoch:003 || batch 000 with loss: 99.750656 
epoch:003 || batch 010 with loss:104.170891 
epoch:003 || batch 020 with loss:124.280502 
epoch:003 || batch 030 with loss: 93.485764 
epoch:003 || batch 040 with loss: 99.092712 
epoch:003 || batch 050 with loss: 99.665649 
epoch:003 || batch 060 with loss: 98.709396 
epoch:003 || batch 070 with loss:109.024902 
epoch:003 || batch 080 with loss: 99.150566 
epoch:003 || batch 090 with loss: 96.575500 
epoch:004 || batch 000 with loss:118.010750 
epoch:004 || batch 010 with loss: 94.216873 
epoch:004 || batch 020 with loss: 98.186737 
epoch:004 || batch 030 with loss: 92.623047 
epoch:004 || batch 040 with loss: 98.520615 
epoch:004 || batch 050 with loss:107.225418 
epoch:004 || batch 060 with loss: 97.849777 
epoch:004 || batch 070 with loss: 93.001648 
epoch:004 || batch 080 with loss: 99.140503 
epoch:004 || batch 090 with loss: 96.415092 
epoch:005 || batch 000 with loss:102.020233 
epoch:005 || batch 010 with loss: 98.334549 
epoch:005 || batch 020 with loss: 93.780869 
epoch:005 || batch 030 with loss: 89.646889 
epoch:005 || batch 040 with loss: 99.565613 
epoch:005 || batch 050 with loss:108.464706 
epoch:005 || batch 060 with loss: 94.033676 
epoch:005 || batch 070 with loss: 98.275192 
epoch:005 || batch 080 with loss: 95.737732 
epoch:005 || batch 090 with loss: 86.539169 
epoch:006 || batch 000 with loss: 98.835327 
epoch:006 || batch 010 with loss:103.602547 
epoch:006 || batch 020 with loss:100.980698 
epoch:006 || batch 030 with loss: 94.003647 
epoch:006 || batch 040 with loss: 95.364532 
epoch:006 || batch 050 with loss:100.852310 
epoch:006 || batch 060 with loss: 93.865616 
epoch:006 || batch 070 with loss: 92.537476 
epoch:006 || batch 080 with loss: 92.078033 
epoch:006 || batch 090 with loss: 95.723389 
epoch:007 || batch 000 with loss: 95.722839 
epoch:007 || batch 010 with loss: 94.787277 
epoch:007 || batch 020 with loss: 96.213387 
epoch:007 || batch 030 with loss: 94.504074 
epoch:007 || batch 040 with loss: 98.739220 
epoch:007 || batch 050 with loss: 94.121132 
epoch:007 || batch 060 with loss: 94.001495 
epoch:007 || batch 070 with loss: 96.122726 
epoch:007 || batch 080 with loss: 98.042831 
epoch:007 || batch 090 with loss:101.341187 
epoch:008 || batch 000 with loss: 99.516518 
epoch:008 || batch 010 with loss:100.057510 
epoch:008 || batch 020 with loss: 90.893768 
epoch:008 || batch 030 with loss: 99.747116 
epoch:008 || batch 040 with loss: 98.303864 
epoch:008 || batch 050 with loss:100.552063 
epoch:008 || batch 060 with loss:102.564301 
epoch:008 || batch 070 with loss: 94.319115 
epoch:008 || batch 080 with loss: 99.027924 
epoch:008 || batch 090 with loss: 95.158516 
epoch:009 || batch 000 with loss: 93.522385 
epoch:009 || batch 010 with loss: 95.134026 
epoch:009 || batch 020 with loss: 98.844543 
epoch:009 || batch 030 with loss: 96.419830 
epoch:009 || batch 040 with loss: 91.699127 
epoch:009 || batch 050 with loss: 85.039062 
epoch:009 || batch 060 with loss: 98.634705 
epoch:009 || batch 070 with loss:102.253708 
epoch:009 || batch 080 with loss: 94.464767 
epoch:009 || batch 090 with loss: 96.967567 
epoch:010 || batch 000 with loss: 95.715454 
epoch:010 || batch 010 with loss: 97.992722 
epoch:010 || batch 020 with loss: 94.299271 
epoch:010 || batch 030 with loss: 95.120110 
epoch:010 || batch 040 with loss: 93.453011 
epoch:010 || batch 050 with loss: 99.340218 
epoch:010 || batch 060 with loss: 98.754623 
epoch:010 || batch 070 with loss: 98.128677 
epoch:010 || batch 080 with loss: 91.893188 
epoch:010 || batch 090 with loss: 89.543327 
epoch:011 || batch 000 with loss: 95.529167 
epoch:011 || batch 010 with loss: 91.078308 
epoch:011 || batch 020 with loss: 86.603844 
epoch:011 || batch 030 with loss: 95.020142 
epoch:011 || batch 040 with loss: 96.768494 
epoch:011 || batch 050 with loss: 94.595642 
epoch:011 || batch 060 with loss: 97.410378 
epoch:011 || batch 070 with loss: 98.031334 
epoch:011 || batch 080 with loss: 96.032120 
epoch:011 || batch 090 with loss: 92.797272 
epoch:012 || batch 000 with loss: 98.379486 
epoch:012 || batch 010 with loss: 92.835831 
epoch:012 || batch 020 with loss: 97.322426 
epoch:012 || batch 030 with loss:100.827454 
epoch:012 || batch 040 with loss: 95.761063 
epoch:012 || batch 050 with loss: 96.827255 
epoch:012 || batch 060 with loss: 95.519531 
epoch:012 || batch 070 with loss: 99.854485 
epoch:012 || batch 080 with loss: 94.881393 
epoch:012 || batch 090 with loss: 88.511665 
epoch:013 || batch 000 with loss: 87.471924 
epoch:013 || batch 010 with loss: 99.858582 
epoch:013 || batch 020 with loss: 97.811050 
epoch:013 || batch 030 with loss: 99.271461 
epoch:013 || batch 040 with loss: 93.671150 
epoch:013 || batch 050 with loss: 98.133896 
epoch:013 || batch 060 with loss: 98.250687 
epoch:013 || batch 070 with loss:101.008713 
epoch:013 || batch 080 with loss: 95.538574 
epoch:013 || batch 090 with loss: 94.794067 
epoch:014 || batch 000 with loss: 93.808578 
epoch:014 || batch 010 with loss: 93.943283 
epoch:014 || batch 020 with loss: 89.392090 
epoch:014 || batch 030 with loss: 95.057381 
epoch:014 || batch 040 with loss: 96.248276 
epoch:014 || batch 050 with loss: 93.808456 
epoch:014 || batch 060 with loss: 96.094574 
epoch:014 || batch 070 with loss: 95.642952 
epoch:014 || batch 080 with loss: 96.181396 
epoch:014 || batch 090 with loss: 95.047531 
epoch:015 || batch 000 with loss: 93.376373 
epoch:015 || batch 010 with loss: 93.833389 
epoch:015 || batch 020 with loss:100.305527 
epoch:015 || batch 030 with loss: 94.328339 
epoch:015 || batch 040 with loss: 92.500755 
epoch:015 || batch 050 with loss: 86.393219 
epoch:015 || batch 060 with loss: 96.430420 
epoch:015 || batch 070 with loss: 98.246597 
epoch:015 || batch 080 with loss: 94.661026 
epoch:015 || batch 090 with loss: 95.271935 
epoch:016 || batch 000 with loss: 95.020294 
epoch:016 || batch 010 with loss: 97.378975 
epoch:016 || batch 020 with loss: 94.723251 
epoch:016 || batch 030 with loss: 95.934700 
epoch:016 || batch 040 with loss:101.353340 
epoch:016 || batch 050 with loss: 99.644531 
epoch:016 || batch 060 with loss: 98.015289 
epoch:016 || batch 070 with loss: 93.249420 
epoch:016 || batch 080 with loss: 84.882027 
epoch:016 || batch 090 with loss: 89.907890 
epoch:017 || batch 000 with loss: 93.837906 
epoch:017 || batch 010 with loss: 96.717064 
epoch:017 || batch 020 with loss: 93.057892 
epoch:017 || batch 030 with loss: 93.553375 
epoch:017 || batch 040 with loss: 95.197754 
epoch:017 || batch 050 with loss: 92.111938 
epoch:017 || batch 060 with loss:100.086205 
epoch:017 || batch 070 with loss: 91.857574 
epoch:017 || batch 080 with loss: 97.159424 
epoch:017 || batch 090 with loss: 98.197792 
/home/zhang_t/.conda/envs/mmwave/lib/python3.9/site-packages/torch/autograd/__init__.py:197: UserWarning: Error detected in MseLossBackward0. Traceback of forward call that caused the error:
  File "/home/zhang_t/workspace/southner/train.py", line 168, in <module>
    main()
  File "/home/zhang_t/workspace/southner/train.py", line 78, in main
    train(e, model, train_data_loader, criterion, optimizer)
  File "/home/zhang_t/workspace/southner/train.py", line 107, in train
    loss = criterion(predict, tag)
  File "/home/zhang_t/.conda/envs/mmwave/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zhang_t/workspace/southner/utils/utils.py", line 145, in forward
    class_loss = F.mse_loss(class_pred,class_target,size_average=False)
  File "/home/zhang_t/.conda/envs/mmwave/lib/python3.9/site-packages/torch/nn/functional.py", line 3292, in mse_loss
    return torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
  File "/home/zhang_t/.conda/envs/mmwave/lib/python3.9/site-packages/torch/fx/traceback.py", line 57, in format_stack
    return traceback.format_stack()
 (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/torch/csrc/autograd/python_anomaly_mode.cpp:114.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
epoch:018 || batch 000 with loss: 93.663254 
epoch:018 || batch 010 with loss: 96.254349 
epoch:018 || batch 020 with loss: 98.750519 
Traceback (most recent call last):
  File "/home/zhang_t/workspace/southner/train.py", line 168, in <module>
    main()
  File "/home/zhang_t/workspace/southner/train.py", line 78, in main
    train(e, model, train_data_loader, criterion, optimizer)
  File "/home/zhang_t/workspace/southner/train.py", line 110, in train
    loss.backward()
  File "/home/zhang_t/.conda/envs/mmwave/lib/python3.9/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/home/zhang_t/.conda/envs/mmwave/lib/python3.9/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Function 'MseLossBackward0' returned nan values in its 0th output.
